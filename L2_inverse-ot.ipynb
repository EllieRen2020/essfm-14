{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Lecture 2: inverse optimal transport</center>\n",
    "### <center>Alfred Galichon (NYU & Sciences Po)</center>\n",
    "## <center>14th European Summer School in Financial Mathematics</center>\n",
    "<center>© 2021 by Alfred Galichon. Past and present support from NSF grant DMS-1716489, ERC grant CoG-866274, and contributions by Jules Baudet, Pauline Corblet, Gregory Dannay, Julie Lenoir, and James Nesbit are acknowledged.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large part of this material is taken from the math+econ+code lecture series:<br>\n",
    "https://www.math-econ-code.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: structural estimation of matching models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "* Matching with unobserved heterogeneities\n",
    "\n",
    "* Estimation of matching models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**[B]** Becker (1973). 'A Theory of Marriage: Part 1.' *Journal of Political Economy*.\n",
    "\n",
    "**[COQ]** Chiappori, Oreffice and Quintana-Domeque (2012). 'Fatter Attraction: Anthropometric and Socioeconomic Matching on the Marriage Market'. *Journal of Political Economy*.\n",
    "\n",
    "**[CS]** Choo and Siow (2006). 'Who Marries Whom and Why'. *Journal of Political Economy*.\n",
    "\n",
    "**[CSW]** Chiappori, Salanié, and Weiss (2017). 'Partner Choice and the Marital College Premium'. * American Economic Review*.\n",
    "\n",
    "**[DG]** Dupuy and Galichon (2014). 'Personality traits and the marriage market'. *Journal of Political Economy*.\n",
    "\n",
    "**[GS]** Galichon and Salanié (2020). 'Cupid's Invisible Hand: Social Surplus and Identification in Matching Models'. Preprint (first version 2011).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation: models of matching since Gary Becker\n",
    "\n",
    "* In the footsteps of Becker, empirical studies on the marriage market had long been focused on one-dimensional models, which assumes that a single index is enough to capture the interactions on the marriage market, and positive assortative matching (PAM), which predicts that the matching equilibrium will tend to match the agents with higher indices with each other.\n",
    "\n",
    "* However, it is desirable to move beyond PAM:\n",
    "    * PAM is always loosely true, never precisely\n",
    "    * there are often many observed characteristics, and it is not always the case that the sorting can be captured by a single-dimensional model\n",
    "    * PAM is a theoretical prediction stemming from assumptions of supermodularity of the surplus function which do not necessarly hold\n",
    "    * optimal transport provide tools to study multidimensional models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, any model of matching based on (unregularized) optimal transport will not be exploitable because it will generate far too strong predictions, namely that some matchings will never hold. This is rather counterfactual: in the data, one observes virtually any combination of type.\n",
    "\n",
    "* Hence, need to regularize the matching model, and we shall do so by introducing unobserved heterogeneity. The model so obtained will be exploitable for estimation and identification purposes. The first such model (with transfers) is the model by [CS]. We shall see a generalization of this model by [GS] (2015).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our libraries\n",
    "\n",
    "We start with loading the libraries we will need. They are rather standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from scipy import optimize\n",
    "# !python -m pip install -i https://pypi.gurobi.com gurobipy ## only if Gurobi not here\n",
    "import gurobipy as grb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A look at our data\n",
    "\n",
    "* Our data are Choo and Siow's original data. Choo and Siow wanted to study the impact of the legalization of abortion by the Roe vs. Wade decision by the supreme court on the 'value of marriage'. Roe vs. Wade decreased the role of marriage in covering out-of-the-wedlocks pregnancies ('shotgun weddings').\n",
    "\n",
    "* The decision did, however, not make a change uniformly in the United\n",
    "States as a number of states had already legalized abortion (reform states).\n",
    "Choo and Siow thus offer a diffs-in-diffs approach in order to compute the\n",
    "change in the value of marriage.\n",
    "\n",
    "* Choo and Siow's data are thus made of the marriages between men and women in reformed states (R) vs nonreformed states (NR), in 1972 and in 1982. One should expect to see a higher drop in marriage value in NR states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "thepath = 'https://raw.githubusercontent.com/math-econ-code/mec_optim_2021-01/master/data_mec_optim/marriage-ChooSiow/'\n",
    "\n",
    "n_singles = pd.read_csv(thepath+'n_singles.txt', sep='\\t', header = None)\n",
    "marr = pd.read_csv(thepath+'marr.txt', sep='\\t', header = None)\n",
    "navail = pd.read_csv(thepath+'n_avail.txt', sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used by Choo and Siow is census data on marriages between age categories, from age 16 (row/column 0) to age 75 (row/age 59). It is thus 60x60 tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22704</td>\n",
       "      <td>10954</td>\n",
       "      <td>3932</td>\n",
       "      <td>1550</td>\n",
       "      <td>672</td>\n",
       "      <td>414</td>\n",
       "      <td>190</td>\n",
       "      <td>114</td>\n",
       "      <td>78</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40266</td>\n",
       "      <td>38980</td>\n",
       "      <td>16368</td>\n",
       "      <td>5714</td>\n",
       "      <td>2116</td>\n",
       "      <td>1101</td>\n",
       "      <td>691</td>\n",
       "      <td>427</td>\n",
       "      <td>260</td>\n",
       "      <td>127</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39219</td>\n",
       "      <td>49753</td>\n",
       "      <td>40315</td>\n",
       "      <td>18001</td>\n",
       "      <td>5401</td>\n",
       "      <td>2291</td>\n",
       "      <td>1429</td>\n",
       "      <td>856</td>\n",
       "      <td>611</td>\n",
       "      <td>338</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29500</td>\n",
       "      <td>44788</td>\n",
       "      <td>47212</td>\n",
       "      <td>39429</td>\n",
       "      <td>16730</td>\n",
       "      <td>6270</td>\n",
       "      <td>3211</td>\n",
       "      <td>1910</td>\n",
       "      <td>762</td>\n",
       "      <td>438</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18952</td>\n",
       "      <td>32860</td>\n",
       "      <td>40103</td>\n",
       "      <td>43488</td>\n",
       "      <td>36265</td>\n",
       "      <td>15318</td>\n",
       "      <td>6076</td>\n",
       "      <td>2872</td>\n",
       "      <td>1552</td>\n",
       "      <td>1246</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5     6     7     8     9   ...  50  \\\n",
       "0  22704  10954   3932   1550    672    414   190   114    78    64  ...   0   \n",
       "1  40266  38980  16368   5714   2116   1101   691   427   260   127  ...   0   \n",
       "2  39219  49753  40315  18001   5401   2291  1429   856   611   338  ...   0   \n",
       "3  29500  44788  47212  39429  16730   6270  3211  1910   762   438  ...   0   \n",
       "4  18952  32860  40103  43488  36265  15318  6076  2872  1552  1246  ...   0   \n",
       "\n",
       "   51  52  53  54  55  56  57  58  59  \n",
       "0   0   0   0   0   0   0   0   0   0  \n",
       "1   0   0   0   0   0   0   0   0   0  \n",
       "2   0   0   0   0   0   0   0   0   0  \n",
       "3   0   0   0   0   0   0   0   0   0  \n",
       "4   0   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data also includes the number of single individuals per age category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010132</td>\n",
       "      <td>907226</td>\n",
       "      <td>772448</td>\n",
       "      <td>597919</td>\n",
       "      <td>454792</td>\n",
       "      <td>341370</td>\n",
       "      <td>315797</td>\n",
       "      <td>253618</td>\n",
       "      <td>171614</td>\n",
       "      <td>152228</td>\n",
       "      <td>...</td>\n",
       "      <td>68383</td>\n",
       "      <td>69177</td>\n",
       "      <td>61358</td>\n",
       "      <td>70752</td>\n",
       "      <td>69112</td>\n",
       "      <td>60909</td>\n",
       "      <td>60749</td>\n",
       "      <td>65302</td>\n",
       "      <td>62922</td>\n",
       "      <td>61117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>790793</td>\n",
       "      <td>671332</td>\n",
       "      <td>602900</td>\n",
       "      <td>492020</td>\n",
       "      <td>404882</td>\n",
       "      <td>313755</td>\n",
       "      <td>255414</td>\n",
       "      <td>195418</td>\n",
       "      <td>141283</td>\n",
       "      <td>128787</td>\n",
       "      <td>...</td>\n",
       "      <td>197456</td>\n",
       "      <td>201183</td>\n",
       "      <td>191519</td>\n",
       "      <td>218031</td>\n",
       "      <td>219157</td>\n",
       "      <td>199926</td>\n",
       "      <td>200052</td>\n",
       "      <td>202146</td>\n",
       "      <td>210315</td>\n",
       "      <td>202775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3       4       5       6       7       8   \\\n",
       "0  1010132  907226  772448  597919  454792  341370  315797  253618  171614   \n",
       "1   790793  671332  602900  492020  404882  313755  255414  195418  141283   \n",
       "\n",
       "       9   ...      50      51      52      53      54      55      56  \\\n",
       "0  152228  ...   68383   69177   61358   70752   69112   60909   60749   \n",
       "1  128787  ...  197456  201183  191519  218031  219157  199926  200052   \n",
       "\n",
       "       57      58      59  \n",
       "0   65302   62922   61117  \n",
       "1  202146  210315  202775  \n",
       "\n",
       "[2 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_singles.transpose().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "The analysis here follows [GS], who build on the logit model by [CS]. \n",
    "\n",
    "* Consider a heterosexual marriage matching market. The set of types (observable characteristics) is $\\mathcal{X}$ for men, and $\\mathcal{Y}$ for women. There are $n_{x}$ men of type $x$, and $m_{y}$ women of type $y$.\n",
    "\n",
    "* Assume that if a man $i\\in\\mathcal{I}$ of type $x_{i}$ and a woman $j\\in\\mathcal{J}$ of type $y_{j}$ match, they get respective utilities \\begin{align*} &  \\alpha_{x_{i}y_{j}}+w_{ij}+\\varepsilon_{iy_{j}}\\\\ &  \\gamma_{x_{i}y_{j}}-w_{ij}+\\eta_{x_{i}j} \\end{align*} where $w_{ij}$ is the transfer from $i$ to $j$. If they remain single $i$ and $j$ get respectively $\\varepsilon_{i0}$ and $\\eta_{0j}$.\n",
    "\n",
    "* The random utility vectors $\\left(  \\varepsilon_{y}\\right)  $ and $\\left(  \\eta_{x}\\right)  $ are drawn from probability distributions $\\mathbf{P}_{x}$ and $\\mathbf{Q}_{y}$, respectively. In the sequel we shall work with a finite number of agents of each type, and then we'll investigate the limit of these results.\n",
    "\n",
    "* The fact that the preferences for the other side of the market terms $\\varepsilon_{iy_{j}}$ and $\\eta_{x_{i}}$ do not vary within observed types is a very important implicit assumption called **separability**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal matching\n",
    "\n",
    "\n",
    "The matching surplus between $i$ and $j$ is therefore $$\\tilde{\\Phi}_{ij}=\\Phi_{x_{i}y_{j}}+\\varepsilon_{iy_{j}}+\\eta_{x_{i}j}$$ where $\\Phi_{xy}=\\alpha_{xy}+\\gamma_{xy}$. The value of optimal matching is thus, under its dual form, \\begin{align*}\\min_{u_{i},v_{j}}  &  \\sum_{i\\in\\mathcal{I}}u_{i}+\\sum_{j\\in\\mathcal{J}}  v_{j}\\\\ s.t.~  &  u_{i}+v_{j}\\geq\\Phi_{x_{i}y_{j}}+\\varepsilon_{iy_{j}}+\\eta_{x_{i}j}\\\\ &  u_{i}\\geq\\varepsilon_{i0}\\\\ &  v_{j}\\geq\\eta_{j0} \\end{align*}\n",
    "\n",
    "Written like this, the lp has $\\left\\vert \\mathcal{I}\\right\\vert +\\left\\vert \\mathcal{J}\\right\\vert $ variables and $\\left\\vert \\mathcal{I} \\right\\vert \\times\\left\\vert \\mathcal{J}\\right\\vert +\\left\\vert \\mathcal{I} \\right\\vert +\\left\\vert \\mathcal{J}\\right\\vert $ constraints. Assuming that there are $K$ individuals per type for each type, this is $K\\left(  \\left\\vert \\mathcal{X}\\right\\vert +\\left\\vert \\mathcal{Y}\\right\\vert \\right)  $ variables and $K^{2}\\left(  \\left\\vert \\mathcal{X}\\right\\vert \\times\\left\\vert \\mathcal{Y}\\right\\vert \\right)  +K\\left(  \\left\\vert \\mathcal{X}\\right\\vert +\\left\\vert \\mathcal{Y}\\right\\vert \\right)  $ constraints.\n",
    "\n",
    "The number of constraints is **quadratic** with respect to $K$. Fortunately, a little thinking about the implications of separability will help us reduce this complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A property of equilibrium\n",
    "\n",
    "We have:\n",
    "\n",
    "---\n",
    "\n",
    "**Lemma**. Consider the set $\\mathcal{I}_{xy}$ of men of type $x$ matched to women of type $y$ at equilibrium. If $\\mathcal{I}_{xy}$ is nonempty, then $u_{i}-\\varepsilon_{iy}$ is a constant across $\\mathcal{I}_{xy}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Proof**. For $i\\in\\mathcal{I}$ such that $x_{i}=x$,\n",
    "\\begin{align*}\n",
    "u_{i}  &  =\\max_{j\\in\\mathcal{J}}\\left\\{  \\tilde{\\Phi}_{ij}-v_{j}%\n",
    ",\\varepsilon_{i0}\\right\\} \\\\\n",
    "&  =\\max_{y\\in\\mathcal{Y}}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon\n",
    "_{i0}\\right\\}\n",
    "\\end{align*}\n",
    "where $U_{xy}=\\max_{j:y_{j}=y}\\left\\{  \\Phi_{xy}+\\eta_{x_{i}j}-v_{j}\\right\\}\n",
    "$, thus $u_{i}\\geq U_{xy}+\\varepsilon_{iy}$ with equality on $\\mathcal{I}%\n",
    "_{xy}$. With similar notations, $v_{j}\\geq V_{xy}+\\eta_{xj}$ with equality on\n",
    "$\\mathcal{J}_{xy}$. As a result, if $\\mathcal{I}_{xy}$ is nonempty, then\n",
    "$U_{xy}+V_{xy}=\\Phi_{xy}$ and $\\forall i\\in\\mathcal{I}_{xy},$ $u_{i}%\n",
    "=U_{xy}+\\varepsilon_{iy}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simplification\n",
    "\n",
    "In the sequel, we shall see that *adding* an auxiliary variable to\n",
    "the previous lp will lead to *decreasing* the computational complexity of\n",
    "the problem.\n",
    "\n",
    "Observe that the first set of constraints is reexpressed by saying that,\n",
    "for every $x\\in\\mathcal{X}$, $y\\in\\mathcal{Y}$,\n",
    "$$\n",
    "\\min_{i:x_{i}=x}\\left\\{  u_{i}-\\varepsilon_{iy}\\right\\}  +\\min_{j:y_{j}\n",
    "=y}\\left\\{  v_{j}-\\eta_{xj}\\right\\}  \\geq\\Phi_{xy}.\n",
    "$$\n",
    "\n",
    "\n",
    "Hence, letting $U_{xy}=\\min_{i:x_{i}=x}\\left\\{  u_{i}-\\varepsilon\n",
    "_{iy}\\right\\}  $ and $V_{xy}=\\min_{j:y_{j}=y}\\left\\{  v_{j}-\\eta_{xj}\\right\\}\n",
    "$, a solution of the previous lp should satisfy\n",
    "$$\n",
    "u_{i}=\\max_{y\\in\\mathcal{Y}}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon\n",
    "_{i0}\\right\\}  \\text{ and }v_{j}=\\max_{x\\in\\mathcal{X}}\\left\\{  V_{xy}\n",
    "+\\varepsilon_{xj},\\varepsilon_{0j}\\right\\}  .\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem rewrites as\n",
    "\\begin{align}\n",
    "\\min_{u_{i},v_{j},U_{xy},V_{xy}}  &  \\sum_{i\\in\\mathcal{I}}u_{i}+\\sum\n",
    "_{j\\in\\mathcal{J}}v_{j}\\label{simplifiedDual}\\\\\n",
    "s.t.~  &  U_{xy}+V_{xy}\\geq\\Phi_{xy}~\\left[  \\mu_{xy}\\geq0\\right] \\nonumber\\\\\n",
    "&  u_{i}\\geq U_{x_{i}y}+\\varepsilon_{iy_{j}}~\\left[  \\mu_{iy}\\right]\n",
    "\\nonumber\\\\\n",
    "&  v_{j}\\geq V_{xy_{j}}+\\eta_{x_{i}j}~\\left[  \\mu_{xi}\\right] \\nonumber\\\\\n",
    "&  u_{i}\\geq\\varepsilon_{i0}~\\left[  \\mu_{i0}\\right] \\nonumber\\\\\n",
    "&  v_{j}\\geq\\eta_{j0}~\\left[  \\mu_{0x}\\right] \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "This problem has $K\\left(  \\left\\vert \\mathcal{X}\\right\\vert +\\left\\vert\n",
    "\\mathcal{Y}\\right\\vert \\right)  +\\left\\vert \\mathcal{X}\\right\\vert\n",
    "\\times\\left\\vert \\mathcal{Y}\\right\\vert $ variables and $\\left(  \\left\\vert\n",
    "\\mathcal{X}\\right\\vert \\times\\left\\vert \\mathcal{Y}\\right\\vert \\right)\n",
    "+K\\left(  2\\left\\vert \\mathcal{X}\\right\\vert \\times\\left\\vert \\mathcal{Y}\n",
    "\\right\\vert +\\left\\vert \\mathcal{X}\\right\\vert +\\left\\vert \\mathcal{Y}\n",
    "\\right\\vert \\right)  $ constraints.\n",
    "\n",
    "The number of constraint is now **linear** with respect to $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consequences\n",
    "\n",
    "**1. Lagrange multipliers:**\n",
    "* The Lagrange multiplier $\\mu_{xy}$ is interpreted as the number of matchings between types $x$ and $y$.\n",
    "\n",
    "* The Lagrange multiplier $\\mu_{iy}$ ($y\\in\\mathcal{Y}_{0}$) is interpreted as a 0-1 indicator that man $i$ chooses a type $y$\n",
    "\n",
    "* The Lagrange multiplier $\\mu_{xj}$ ($x\\in\\mathcal{X}_{0}$) is interpreted as a 0-1 indicator that woman $j$ chooses a\\ type $x$\n",
    "\n",
    "**2. Utilities:**\n",
    "* Man $i$ solves a discrete choice problem $u_{i}=\\max_{y\\in\\mathcal{Y}\n",
    "}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon_{i0}\\right\\}  $\n",
    "\n",
    "* Woman $j$ solve a discrete choice problem $v_{j}=\\max_{x\\in\\mathcal{X}\n",
    "}\\left\\{  V_{xy}+\\eta_{xj},\\eta_{0j}\\right\\}  .$\n",
    "\\end{itemize}\n",
    "\n",
    "* $U_{xy}$ and $V_{xy}$ are related by $U_{xy}+V_{xy}\\geq\\Phi_{xy}$ with\n",
    "equality if $\\mu_{xy}>0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large market limit\n",
    "\n",
    "Now look at the limit of previous markets when the number of market participants gets large, holding fixed the frequency of each types.\n",
    "\n",
    "In the large population limit $n_{x}$ and $m_{y}$ are now interpreted as the mass distribution of respective types $x$ and $y$.\n",
    "\n",
    "We shall from now on assume that $\\mathbf{P}_{x}$ and $\\mathbf{Q}_{y}$, the distributions of random utility vectors $\\left( \\varepsilon_{y}\\right)  $ and $\\left(  \\eta_{x}\\right)  $, have a density with full support. This will ensure that the Emax operators associated with the choice problems of the men and the women respectively \\begin{align*} & G_x(U_{x.}) = \\mathbb{E}_\\mathbf{P} \\left[\\max_{y\\in\\mathcal{Y}\n",
    "}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon_{i0}\\right\\} \\right]\\text{, and }\\\\ & H_y(V_{.y}) = \\mathbb{E}_\\mathbf{Q} \\left[\\max_{x\\in\\mathcal{X}\n",
    "}\\left\\{  V_{xy}+\\eta_{xj},\\eta_{0j}\\right\\} \\right],\\end{align*}as well as the corresponding entropies of choice $G_x^{\\ast}$ and $H_y^{\\ast}$ are continuously differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under these assumptions, the problem becomes\n",
    "\\begin{align*}\n",
    "\\min_{U,V} ~&  G\\left(  U\\right)  +H\\left(  V\\right) \\\\\n",
    "s.t.~  &  U_{xy}+V_{xy}\\geq\\Phi_{xy}~\\left[  \\mu_{xy}\\right]\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "G\\left(  U\\right)   &  =\\sum_{x\\in\\mathcal{X}}n_{x}\\mathbb{E}_{\\mathbf{P}%\n",
    "}\\left[  \\max_{y\\in\\mathcal{Y}}\\left\\{  U_{xy}+\\varepsilon_{iy},\\varepsilon\n",
    "_{i0}\\right\\}  \\right] \\\\\n",
    "H\\left(  V\\right)   &  =\\sum_{y\\in\\mathcal{Y}}m_{y}\\mathbb{E}_{\\mathbf{Q}%\n",
    "}\\left[  \\max_{x\\in\\mathcal{X}}\\left\\{  V_{xy}+\\eta_{xj},\\eta_{0j}\\right\\}\n",
    "\\right]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "By first order conditions,\n",
    "$$\n",
    "\\frac{\\partial G\\left(  U\\right)  }{\\partial U_{xy}}=\\mu_{xy}=\\frac{\\partial\n",
    "H\\left(  V\\right)  }{\\partial V_{xy}}.\n",
    "$$\n",
    "and $\\mu_{xy}>0$ for every $x\\in\\mathcal{X}$ and $y\\in\\mathcal{Y}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social planner's problem\n",
    "\n",
    "The primal problem corresponding the problem above is\n",
    "$$\n",
    "\\max_{\\mu_{xy}\\geq0}\\sum_{\\substack{x\\in\\mathcal{X}\\\\y\\in\\mathcal{Y}}}\\mu\n",
    "_{xy}\\Phi_{xy}-\\mathcal{E}\\left(  \\mu\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathcal{E}\\left(  \\mu\\right)  =G^{\\ast}\\left(  \\mu\\right)  +H^{\\ast}\\left(\n",
    "\\mu\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Recall $G^{\\ast}\\left(  \\mu\\right)  =\\max\\left\\{  \\sum_{xy}\\mu\n",
    "_{xy}U_{xy}-G\\left(  U\\right)  \\right\\}  $ is the Legendre transform of $G$, and similarly for $H^{\\ast}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of the matching surplus\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem.** By first order conditions, we get the identifcation formula of $\\Phi$%\n",
    "$$\n",
    "\\Phi_{xy}=\\frac{\\partial G^{\\ast}\\left(  \\mu\\right)  }{\\partial\\mu_{xy}}\n",
    "+\\frac{\\partial H^{\\ast}\\left(  \\mu\\right)  }{\\partial\\mu_{xy}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "This means that the surplus function is identified *nonparametrically* given the matching patterns $\\mu$ and assuming a fixed distribution of unobserved heterogeneity.\n",
    "\n",
    "Hence only the joint surplus $\\Phi\n",
    "_{xy}=\\alpha_{xy}+\\gamma_{xy}$ is identified. However, if the transfers\n",
    "$\\hat{w}_{xy}$ are observed too (e.g. wages in labour market), then\n",
    "$U_{xy}=\\alpha_{xy}+w_{xy}$ and $V_{xy}=\\gamma_{xy}-w_{xy}$, so that $\\alpha$\n",
    "and $\\gamma$ are separately identified by\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}%\n",
    "\\hat{\\alpha}_{xy}=\\frac{\\partial G^{\\ast}\\left(  \\mu\\right)  }{\\partial\\mu_{xy}}-\\hat{w}_{xy}\\\\\n",
    "\\hat{\\gamma}_{xy}=\\frac{\\partial H^{\\ast}\\left(  \\mu\\right)  }{\\partial\\mu_{xy}}+\\hat{w}_{xy}%\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choo and Siow's logit model\n",
    "\n",
    "In Choo and Siow's model [CS], the heterogeneities in tastes are Gubmel,\n",
    "we have\n",
    "$$\n",
    "\\mathcal{E}\\left(  \\mu\\right)  =2\\sum_{\\substack{x\\in\\mathcal{X}%\n",
    "\\\\y\\in\\mathcal{Y}}}\\mu_{xy}\\log\\mu_{xy}+\\sum_{x\\in\\mathcal{X}}\\mu_{x0}\\log\n",
    "\\mu_{x0}+\\sum_{y\\in\\mathcal{Y}}\\mu_{0y}\\log\\mu_{0y}.\n",
    "$$\n",
    "Note that $\\mathcal{E}\\left(  \\mu\\right)  < + \\infty$ if and only if $\\mu\n",
    "\\in\\mathcal{M}\\left(  n,m\\right)  $.\n",
    "\n",
    "\n",
    "By first order conditions above, Choo-Siow's TU-logit model implies the\n",
    "following matching function:\n",
    "$$\n",
    "\\mu_{xy}=M_{xy}\\left(  \\mu_{x0},\\mu_{0y}\\right)  :=\\sqrt{\\mu_{x0}}\\sqrt\n",
    "{\\mu_{0y}}\\exp\\left(  \\frac{\\Phi_{xy}}{2}\\right) \n",
    "$$\n",
    "\n",
    "This is a gravity equation of sorts. The full link with gravity equations is explored in the next lecture. \n",
    "\n",
    "As a result, $\\partial\\mathcal{E}\\left(  \\mu\\right)  /\\partial\\mu\n",
    "_{xy}=2\\log\\mu_{xy}-\\log\\mu_{x0}-\\log\\mu_{0y}$, which implies that $\\Phi_{xy}$\n",
    "is estimated by *Choo and Siow's identification formula*\n",
    "$$\n",
    "\\hat{\\Phi}_{xy}=\\log\\frac{\\hat{\\mu}_{xy}^{2}}{\\hat{\\mu}_{x0}\\hat{\\mu}_{0y}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving equilibrium in the Choo-Siow model\n",
    "\n",
    "Write down the equilibrium equations in the TU-logit model:\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\sum_{y\\in\\mathcal{Y}}\\sqrt{\\mu_{x0}}\\sqrt{\\mu_{0y}}\\exp\\left(  \\frac\n",
    "{\\Phi_{xy}}{2}\\right)  +\\mu_{x0}=n_{x}\\\\\n",
    "\\sum_{x\\in\\mathcal{X}}\\sqrt{\\mu_{x0}}\\sqrt{\\mu_{0y}}\\exp\\left(  \\frac\n",
    "{\\Phi_{xy}}{2}\\right)  +\\mu_{0y}=m_{y}%\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Setting $a_{x}=\\sqrt{\\mu_{x0}}$, $b_{y}=\\sqrt{\\mu_{0y}}$, and\n",
    "$K_{xy}=\\exp\\left(  \\Phi_{xy}/2\\right)  $, this rewrites as\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\sum_{y\\in\\mathcal{Y}}K_{xy}a_{x}b_{y}+a_{x}^{2}=n_{x}\\\\\n",
    "\\sum_{x\\in\\mathcal{X}}K_{xy}a_{x}b_{y}+b_{y}^{2}=m_{y}%\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "which is a variant of the equations previously seen to accomodate unmatched agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily adapt the IPFP to this setting. The IPFP will consists in iteratively solving quadratic equations:\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{l}\n",
    "a_{x}^{2t+1}=\\sqrt{n_{x}+\\left(  \\sum_{y\\in\\mathcal{Y}}b_{y}^{2t}%\n",
    "K_{xy}/2\\right)  ^{2}}-\\sum_{y\\in\\mathcal{Y}}b_{y}^{2t}K_{xy}/2\\\\\n",
    "b_{y}^{2t+2}=\\sqrt{m_{y}+\\left(  \\sum_{x\\in\\mathcal{X}}a_{x}^{2t+1}%\n",
    "K_{xy}/2\\right)  ^{2}}-\\sum_{x\\in\\mathcal{X}}a_{x}^{2t+1}K_{xy}/2\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual problem\n",
    "\n",
    "The dual problem is given by\n",
    "$$\n",
    "\\min_{u,v}\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}%\n",
    "\\sum_{x}n_{x}u_{x}+\\sum_{y}m_{y}v_{y}\\\\\n",
    "+2\\sum_{xy}\\sqrt{n_{x}m_{y}}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{2}\\right)\n",
    "\\\\\n",
    "+\\sum_{x}n_{x}\\exp\\left(  -u_{x}\\right)  +\\sum_{y}m_{y}\\exp\\left(\n",
    "-v_{y}\\right)\n",
    "\\end{array}\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "**Remarks.**\n",
    "\n",
    "* This problem is an unconstrained convex optimization problem, so this formulation will be quite useful.\n",
    "\n",
    "* If $\\left(  u,v\\right)  $ is solution, $u_{x}=-\\log\\mu_{0|x}=-\\log\\left(  \\mu_{x0}/n_{x}\\right)  $ and $v_{y}=-\\log\\left(  \\mu\n",
    "_{0|y}\\right)  $.\n",
    "\n",
    "* Note that the IPFP algorithm just seen interprets as (blockwise) *coordinate descent* method in the dual problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another application: estimation of affinity matrix\n",
    "\n",
    "Dupuy and G (2014) focus on cross-dimensional interactions\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi_{xy}^{A}=\\sum_{p,q}A_{pq}\\xi_{x}^{p}\\xi_{y}^{q}\n",
    "\\end{align*}\n",
    "\n",
    "and estimate \"affinity matrix\" $A$ on a dataset of married individuals where the \"big 5\" personality traits are measured.\n",
    "\n",
    "$A$ is estimated by\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{s_{i},m_{n}}\\min_{A}\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}%\n",
    "\\sum_{x}p_{x}u_{x}+\\sum_{y}q_{y}v_{y}\\\\\n",
    "+\\sum_{xy}\\exp\\left(  \\sum_{p,q}A_{pq}\\xi_{x}^{p}\\xi_{y}^{q}-u_{x}%\n",
    "-v_{y}\\right) \\\\\n",
    "-\\sum_{x,y,p,q}\\hat{\\pi}_{xy}A_{pq}\\xi_{x}^{p}\\xi_{y}^{q}%\n",
    "\\end{array}\n",
    "\\right\\}  .\n",
    "\\end{align*}\n",
    "\n",
    "Dupuy, Galichon and Sun (2016) consider the case when the space of characteristics is high-dimensional. More on this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of affinity matrix: results\n",
    "\n",
    "|  Husbands   \\ Wives             | Education | Height | BMI   | Health | Consc. | Extra. | Agree | Emotio | Auto. | Risk  |\n",
    "|-------------------|-----------|--------|-------|--------|--------|--------|-------|--------|-------|-------|\n",
    "| Education         | 0.46      | 0      | -0.06 | 0.01   | -0.02  | 0.03   | -0.01 | -0.03  | 0.04  | 0.01  |\n",
    "| Height            | 0.04      | 0.21   | 0.04  | 0.03   | -0.06  | 0.03   | 0.02  | 0      | -0.01 | 0.02  |\n",
    "| BMI               | -0.03     | 0.03   | 0.21  | 0.01   | 0.03   | 0      | -0.05 | 0.02   | 0.01  | -0.02 |\n",
    "| Health            | -0.02     | 0.02   | -0.04 | 0.17   | -0.04  | 0.02   | -0.01 | 0.01   | 0     | 0.03  |\n",
    "| Conscienciousness | -0.07     | -0.01  | 0.07  | 0      | 0.16   | 0.05   | 0.04  | 0.06   | 0.01  | 0.01  |\n",
    "| Extraversion      | 0         | -0.01  | 0     | 0.01   | -0.06  | 0.08   | -0.04 | -0.01  | 0.02  | -0.06 |\n",
    "| Agreeableness     | 0.01      | 0.01   | -0.06 | 0.02   | 0.1    | -0.11  | 0     | 0.07   | -0.07 | -0.05 |\n",
    "| Emotional         | 0.03      | -0.01  | 0.04  | 0.06   | 0.19   | 0.04   | 0.01  | -0.04  | 0.08  | 0.05  |\n",
    "| Autonomy          | 0.03      | 0.02   | 0.01  | 0.02   | -0.09  | 0.09   | -0.04 | 0.02   | -0.1  | 0.03  |\n",
    "| Risk              | 0.03      | -0.01  | -0.03 | -0.01  | 0      | -0.02  | -0.03 | -0.03  | 0.08  | 0.14  |\n",
    "\n",
    "Affinity matrix. Source: Dupuy and G (2014). Note: Bold coefficients are significant at the 5 percent level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbCateg = 25\n",
    "\n",
    "muhat_x0 = n_singles[0].iloc[0:nbCateg]\n",
    "muhat_0y = n_singles[1].iloc[0:nbCateg]\n",
    "muhat_xy = marr.iloc[0:nbCateg:,0:nbCateg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nh = muhat_xy.values.sum()+muhat_x0.sum()+muhat_0y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14885023"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*muhat_xy.values.sum()+muhat_x0.sum()+muhat_0y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "muhat_xy = muhat_xy / Nh \n",
    "muhat_x0 = muhat_x0 / Nh \n",
    "muhat_0y = muhat_0y / Nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = muhat_xy.sum(axis = 1)+muhat_x0\n",
    "m_y = muhat_xy.sum(axis = 0)+muhat_0y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbX = nbCateg\n",
    "nbY = nbCateg\n",
    "\n",
    "xs = np.repeat(range(1,nbX+1),nbY).reshape(nbX,nbY)/25\n",
    "ys = np.repeat(range(1,nbY+1),nbX).reshape(nbX,nbY).T/25\n",
    "\n",
    "phi1_xy = -((xs-ys)**2).flatten()\n",
    "phimat = np.column_stack((phi1_xy,np.multiply(phi1_xy,(((xs+ys)/2)**2).flatten()),np.multiply(phi1_xy,(((xs+ys-2)/2)**2).flatten()),np.multiply(phi1_xy,((xs+ys-1)**2).flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbK = phimat.shape[1]\n",
    "phimat_mean = phimat.mean(axis = 0)\n",
    "phimat_stdev = phimat.std(axis = 0, ddof = 1)\n",
    "phimat = ((phimat - phimat_mean).T/phimat_stdev[:,None]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObjFunc(uvlambda):\n",
    "    u_x = uvlambda[0:nbX]\n",
    "    v_y = uvlambda[nbX:(nbX+nbY)]\n",
    "    l = uvlambda[(nbX+nbY):(nbX+nbY+nbK)]\n",
    "    \n",
    "    Phi_xy = phimat.dot(l.reshape(nbK,1)).reshape(nbX, nbY)\n",
    "    mu_xy = np.exp(((Phi_xy - u_x).T-v_y).T/2)\n",
    "    mu_x0 = np.exp(-u_x)\n",
    "    mu_0y = np.exp(-v_y)\n",
    "    \n",
    "    val = sum(np.multiply(n_x,u_x))+sum(np.multiply(m_y,v_y))-np.sum(np.multiply(muhat_xy.values,Phi_xy), axis = (0,1)) + 2*np.sum(mu_xy, axis =(0,1)) + sum(mu_x0) + sum(mu_0y)\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_ObjFunc(uvlambda):\n",
    "    u_x = uvlambda[0:nbX]\n",
    "    v_y = uvlambda[nbX:(nbX+nbY)]\n",
    "    l = uvlambda[(nbX+nbY):(nbX+nbY+nbK)]\n",
    "    \n",
    "    Phi_xy = phimat.dot(l.reshape(nbK,1)).reshape(nbX, nbY)\n",
    "    mu_xy = np.exp(((Phi_xy - u_x).T-v_y).T/2)\n",
    "    mu_x0 = np.exp(-u_x)\n",
    "    mu_0y = np.exp(-v_y)\n",
    "    \n",
    "    grad_u = n_x - np.sum(mu_xy, axis = 0) - mu_x0\n",
    "    grad_v = m_y - np.sum(mu_xy, axis = 1) - mu_0y\n",
    "    grad_lambda = (mu_xy-muhat_xy.values).flatten()[:,None].T.dot(phimat)\n",
    "    \n",
    "    grad = np.concatenate((grad_u,grad_v,grad_lambda.flatten()))\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = optimize.minimize(ObjFunc,method = 'CG',jac = grad_ObjFunc, x0 = np.repeat(0,nbX+nbY+nbK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 7.677025691402801\n",
      "     jac: array([ 4.88075801e-06,  5.10780308e-08, -5.40814185e-08, -9.67674359e-09,\n",
      "       -3.27666407e-08, -5.03933471e-07, -1.01204451e-07, -6.63029291e-08,\n",
      "       -6.00233391e-08, -5.64389548e-07,  9.76712940e-07,  1.22520487e-06,\n",
      "        1.24756462e-06,  1.27584707e-06,  6.88245950e-07,  2.75181310e-07,\n",
      "       -1.86412480e-07,  3.47244011e-07,  7.76230370e-07,  4.02940556e-08,\n",
      "       -6.83570777e-07, -1.13239671e-06, -1.48543343e-06, -1.18186292e-06,\n",
      "       -1.98637193e-06,  1.71723629e-06,  8.02963383e-07,  2.59413708e-07,\n",
      "       -1.08526144e-07, -7.80600168e-07, -1.05835123e-06,  4.97217929e-07,\n",
      "        1.18610346e-07,  1.95591308e-06,  2.88970690e-07,  5.54150734e-07,\n",
      "        3.99675416e-07,  2.02151631e-06,  5.66475382e-07,  1.15734675e-06,\n",
      "        2.94606296e-07,  1.68397986e-07,  1.32991993e-07,  1.41378739e-07,\n",
      "       -3.87513667e-08, -2.92367924e-07, -1.25325454e-06, -1.78125071e-06,\n",
      "       -1.47750538e-06, -4.15938391e-07, -1.59006568e-07,  5.70250050e-07,\n",
      "        1.84558603e-08,  7.05821237e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 421\n",
      "     nit: 162\n",
      "    njev: 421\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 3.11102542,  3.1942911 ,  3.35898889,  3.65697275,  4.01385288,\n",
      "        4.43474088,  4.69390301,  5.03868869,  5.67779702,  5.82894836,\n",
      "        5.95965023,  5.82470143,  6.07519871,  6.31489561,  6.38473279,\n",
      "        6.75213323,  7.3382807 ,  8.07538348,  9.44777863, 11.01220163,\n",
      "       13.31827783, 16.13181931, 19.71382466, 23.80260303, 28.92590982,\n",
      "        3.3060289 ,  3.43944638,  3.62095975,  3.94450684,  4.31881146,\n",
      "        4.83525862,  5.31413153,  5.77560961,  6.3814156 ,  6.48732659,\n",
      "        6.45792254,  6.28543211,  6.49701562,  6.5526705 ,  6.49530694,\n",
      "        6.99358023,  7.44582699,  8.2771778 ,  9.4068751 , 11.08136243,\n",
      "       13.43639185, 16.07694629, 19.68768474, 23.83746151, 29.01399273,\n",
      "       -1.36756245, -6.4086041 ,  4.60390741, -1.3049116 ])\n",
      "\n",
      "7.677025691402801\n",
      "[-1.36756245 -6.4086041   4.60390741 -1.3049116 ]\n"
     ]
    }
   ],
   "source": [
    "uvlambdahat =  outcome['x']\n",
    "lambdahat = uvlambdahat[(nbX+nbY):(nbX+nbY+nbK)]\n",
    "print(outcome)\n",
    "print(\"\")\n",
    "print(ObjFunc(uvlambdahat))\n",
    "print(lambdahat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.36756245 -6.4086041   4.60390741 -1.3049116 ]\n"
     ]
    }
   ],
   "source": [
    "print(lambdahat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: the gravity equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Learning objectives\n",
    "\n",
    "* Regularized optimal transport\n",
    "\n",
    "* The gravity equation\n",
    "\n",
    "* Generalized linear models\n",
    "\n",
    "* Pseudo-Poisson maximum likelihood estimation\n",
    "\n",
    "### References\n",
    "\n",
    "* Anderson and van Wincoop (2003). \"Gravity with Gravitas: A Solution to the Border Puzzle\". *American Economic Review*.\n",
    "\n",
    "* Head and Mayer (2014). \"Gravity Equations: Workhorse, Toolkit and Cookbook\". *Handbook of International Economics*.\n",
    "\n",
    "* Choo and Siow (2005). \"Who marries whom and why\". *Journal of Political Economy*.\n",
    "\n",
    "* Gourieroux, Trognon, Monfort (1984). \"Pseudo Maximum Likelihood Methods: Theory\". *Econometrica*.\n",
    "\n",
    "* McCullagh and Nelder (1989). *Generalized Linear Models*. Chapman and Hall/CRC.\n",
    "\n",
    "* Santos Silva and Tenreyro (2006). \"The Log of Gravity\". *Review of Economics and Statistics*.\n",
    "\n",
    "* Yotov et al. (2011). *An advanced guide to trade policy analysis*. WTO.\n",
    "\n",
    "* Guimares and Portugal (2012). \"Real Wages and the Business Cycle: Accounting for Worker, Firm, and Job Title Heterogeneity\". *AEJ: Macro*.\n",
    "\n",
    "* Dupuy and G (2014), \"Personality traits and the marriage market\". *Journal of Political Economy*.\n",
    "\n",
    "* Dupuy, G and Sun (2019), \"Estimating matching affinity matrix under low-rank constraints\". *Information and Inference*.\n",
    "\n",
    "* Carlier, Dupuy, Galichon and Sun \"SISTA: learning optimal transport costs under sparsity constraints.\" *Communications on Pure and Applied Mathematics* (forthcoming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "The gravity equation is a very useful tool for explaining trade flows by various measures of proximity between countries.\n",
    "\n",
    "A number of regressors have been proposed. They include: geographic distance, common official languague, common colonial past, share of common religions, etc.\n",
    "\n",
    "The dependent variable is the volume of exports from country $i$ to country $n$, for each pair of country $\\left(  i,n\\right)$.\n",
    "\n",
    "Today, we shall see a close connection between gravity models of international trade and separable matching models.\n",
    "\n",
    "---\n",
    "\n",
    "To start with, let's load some of the libraries we shall need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as str\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's load our data, which comes from the book *An Advanced Guide to Trade Policy Analysis: The Structural Gravity Mode*, by Yotov et al. We will estimate the gravity model using optimal transport as well as using Poisson regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exporter</th>\n",
       "      <th>importer</th>\n",
       "      <th>year</th>\n",
       "      <th>trade</th>\n",
       "      <th>DIST</th>\n",
       "      <th>ln_DIST</th>\n",
       "      <th>CNTG</th>\n",
       "      <th>LANG</th>\n",
       "      <th>CLNY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARG</td>\n",
       "      <td>ARG</td>\n",
       "      <td>1986</td>\n",
       "      <td>61288.590263</td>\n",
       "      <td>533.908240</td>\n",
       "      <td>6.280224</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARG</td>\n",
       "      <td>AUS</td>\n",
       "      <td>1986</td>\n",
       "      <td>27.764874</td>\n",
       "      <td>12044.574134</td>\n",
       "      <td>9.396370</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARG</td>\n",
       "      <td>AUT</td>\n",
       "      <td>1986</td>\n",
       "      <td>3.559843</td>\n",
       "      <td>11751.146521</td>\n",
       "      <td>9.371706</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARG</td>\n",
       "      <td>BEL</td>\n",
       "      <td>1986</td>\n",
       "      <td>96.102567</td>\n",
       "      <td>11305.285764</td>\n",
       "      <td>9.333026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARG</td>\n",
       "      <td>BGR</td>\n",
       "      <td>1986</td>\n",
       "      <td>3.129231</td>\n",
       "      <td>12115.572046</td>\n",
       "      <td>9.402246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exporter importer  year         trade          DIST   ln_DIST  CNTG  LANG  \\\n",
       "0      ARG      ARG  1986  61288.590263    533.908240  6.280224     0     0   \n",
       "1      ARG      AUS  1986     27.764874  12044.574134  9.396370     0     0   \n",
       "2      ARG      AUT  1986      3.559843  11751.146521  9.371706     0     0   \n",
       "3      ARG      BEL  1986     96.102567  11305.285764  9.333026     0     0   \n",
       "4      ARG      BGR  1986      3.129231  12115.572046  9.402246     0     0   \n",
       "\n",
       "   CLNY  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thepath = 'https://raw.githubusercontent.com/math-econ-code/mec_optim_2021-01/master/data_mec_optim/gravity_wtodata/'\n",
    "tradedata = pd.read_csv(thepath +'1_TraditionalGravity_from_WTO_book.csv', sep=',')\n",
    "#tradedata = pd.read_csv(os.path.join(thepath ,'1_TraditionalGravity_from_WTO_book.csv'), sep=',')\n",
    "tradedata = tradedata[['exporter', 'importer','year', 'trade', 'DIST','ln_DIST', 'CNTG', 'LANG', 'CLNY']]\n",
    "tradedata.sort_values(['year','exporter','importer'], inplace = True)\n",
    "tradedata.reset_index(inplace = True, drop = True)\n",
    "\n",
    "nbt = len(tradedata['year'].unique())\n",
    "nbi = len(tradedata['importer'].unique())\n",
    "nbk = 4\n",
    "\n",
    "tradedata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent the common practice, we only look at the flows of trade between pairs of distinct countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradedata.loc[np.where(tradedata['importer']==tradedata['exporter'],True, False),['DIST', 'ln_DIST', 'CNTG', 'LANG', 'CLNY']]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data so that we can use it. We want to construct \n",
    "* $D_{ni,t}^k$ which is the $k$th pairwise discrepancy measure between importer $n$ and exporter $i$ at time $t$\n",
    "\n",
    "* $X_{n,t}$ total value of expenditure of importer $n$ at time $t$\n",
    " \n",
    "* $Y_{i,t}$ total value of production of exporter $i$ at time $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhatnit = []\n",
    "Dnikt = []\n",
    "\n",
    "years = tradedata['year'].unique()\n",
    "for t, year in enumerate(years):\n",
    "    \n",
    "    tradedata_year = tradedata[tradedata['year']==year]\n",
    "    \n",
    "    Xhatnit.append(tradedata_year.pivot(index = 'exporter', columns = 'importer', values ='trade').values)\n",
    "    np.fill_diagonal(Xhatnit[t],0)\n",
    "    \n",
    "    Dnikt.append(tradedata_year[[ 'ln_DIST', 'CNTG', 'LANG', 'CLNY']].values)\n",
    "    \n",
    "Xnt = np.zeros((nbi,nbt))\n",
    "Yit = np.zeros((nbi,nbt))\n",
    "\n",
    "for t in range(nbt):\n",
    "    Xnt[:,t] = Xhatnit[t].sum(axis = 1)\n",
    "    Yit[:,t] = Xhatnit[t].sum(axis = 0)\n",
    "    \n",
    "totalmass_t = sum(Xhatnit).sum(axis=(0,1))/nbt\n",
    "pihat_nit = Xhatnit/totalmass_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's standardize the data and construct some useful objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanD_k = np.asmatrix([mat.mean(axis = 0) for mat in Dnikt]).mean(axis = 0)\n",
    "sdD_k = np.asmatrix([mat.std(axis = 0,ddof = 1) for mat in Dnikt]).mean(axis = 0)\n",
    "\n",
    "Dnikt = [(mat - meanD_k)/sdD_k for mat in Dnikt]\n",
    "\n",
    "p_nt = Xnt/totalmass_t\n",
    "q_nt = Yit/totalmass_t\n",
    "IX = np.repeat(1, nbi).reshape(nbi,1)\n",
    "tIY = np.repeat(1, nbi).reshape(1,nbi)\n",
    "\n",
    "f_nit = []\n",
    "g_nit = []\n",
    "\n",
    "for t in range(nbt):\n",
    "    f_nit.append(p_nt[:,t].reshape(nbi,1).dot(tIY))\n",
    "    g_nit.append(IX.dot(q_nt[:,t].reshape(1,nbi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized optimal transport\n",
    "\n",
    "Consider the optimal transport duality\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\pi\\in\\mathcal{M}\\left(  P,Q\\right)  }\\sum_{xy}\\pi_{xy}\\Phi_{xy}=\\min_{u_{x}+v_{y}\\geq\\Phi_{xy}}\\sum_{x\\in\\mathcal{X}}p_{x}u_{x}+\\sum_{y\\in\\mathcal{Y}}q_{y}v_{y}\n",
    "\\end{align*}\n",
    "\n",
    "Now let's assume that we are adding an entropy to the primal objective function. For any $\\sigma>0$, we get\n",
    "\n",
    "\\begin{align*}\n",
    "&  \\max_{\\pi\\in\\mathcal{M}\\left(  P,Q\\right)  }\\sum_{xy}\\pi_{xy}\\Phi_{xy}-\\sigma\\sum_{xy}\\pi_{xy}\\ln\\pi_{xy}\\\\\n",
    "&  =\\min_{u,v}\\sum_{x\\in\\mathcal{X}}p_{x}u_{x}+\\sum_{y\\in\\mathcal{Y}}q_{y}v_{y}+\\sigma\\sum_{xy}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}-\\sigma}{\\sigma}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "The latter problem is an unconstrained convex optimization problem. But the most efficient numerical computation technique is often coordinate descent, i.e. alternate between minimization in $u$ and minimization in $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterated fitting\n",
    "\n",
    "Maximize wrt to $u$ yields\n",
    "\n",
    "\\begin{align*}\n",
    "e^{-u_{x}/\\sigma}=\\frac{p_{x}}{\\sum_{y}\\exp\\left(  \\frac{\\Phi_{xy}-v_{y}-\\sigma}{\\sigma}\\right)  }\n",
    "\\end{align*}\n",
    "\n",
    "and wrt $v$ yields\n",
    "\n",
    "\\begin{align*}\n",
    "e^{-v_{y}/\\sigma}=\\frac{q_{y}}{\\sum_{x}\\exp\\left(  \\frac{\\Phi_{xy}-v_{y}-\\sigma}{\\sigma}\\right)  }\n",
    "\\end{align*}\n",
    "\n",
    "It is called the \"iterated projection fitting procedure\" (ipfp), aka \"matrix scaling\", \"RAS algorithm\", \"Sinkhorn-Knopp algorithm\", \"Kruithof's method\", \"Furness procedure\", \"biproportional fitting procedure\", \"Bregman's procedure\". See survey in Idel (2016).\n",
    "\n",
    "Maybe the most often reinvented algorithm in applied mathematics. Recently rediscovered in a machine learning context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Econometrics of matching\n",
    "\n",
    "The goal is to estimate the matching surplus $\\Phi_{xy}$. For this, take a linear parameterization\n",
    "\n",
    "\\begin{align*}\n",
    "\\Phi_{xy}^{\\beta}=\\sum_{k=1}^{K}\\beta_{k}\\phi_{xy}^{k}.\n",
    "\\end{align*}\n",
    "\n",
    "Following Choo and Siow (2006), Galichon and Salanie (2011) introduce logit heterogeneity in individual preferences and show that the equilibrium now maximizes the *regularized Monge-Kantorovich problem*\n",
    "\n",
    "\\begin{align*}\n",
    "W\\left(  \\beta\\right)  =\\max_{\\pi\\in\\mathcal{M}\\left(  P,Q\\right)  }\\sum_{xy}\\pi_{xy}\\Phi_{xy}^{\\beta}-\\sigma\\sum_{xy}\\pi_{xy}\\ln\\pi_{xy}\n",
    "\\end{align*}\n",
    "\n",
    "By duality, $W\\left(  \\beta\\right)  $ can be expressed\n",
    "\n",
    "\\begin{align*}\n",
    "W\\left(  \\beta\\right)  =\\min_{u,v}\\sum_{x}p_{x}u_{x}+\\sum_{y}q_{y}v_{y}+\\sigma\\sum_{xy}\\exp\\left(  \\frac{\\Phi_{xy}^{\\beta}-u_{x}-v_{y}-\\sigma}{\\sigma}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "and w.l.o.g. can set $\\sigma=1$ and drop the additive constant $-\\sigma$ in the $\\exp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "\n",
    "We observe the actual matching $\\hat{\\pi}_{xy}$. Note that $\\partial W/ \\partial\\beta^{k}=\\sum_{xy}\\pi_{xy}\\phi_{xy}^{k},$\\ hence $\\beta$ is estimated by running\n",
    "\n",
    "<a name='objFun'></a>\n",
    "\\begin{align*}\n",
    "\\min_{u,v,\\beta}\\sum_{x}p_{x}u_{x}+\\sum_{y}q_{y}v_{y}+\\sum_{xy}\\exp\\left(\\Phi_{xy}^{\\beta}-u_{x}-v_{y}\\right)  -\\sum_{xy,k}\\hat{\\pi}_{xy}\\beta_{k}\\phi_{xy}^{k}\n",
    "\\end{align*}\n",
    "\n",
    "which is still a convex optimization problem.\n",
    "\n",
    "This is actually the objective function of the log-likelihood in a Poisson regression with $x$ and $y$ fixed effects, where we assume\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{xy}|xy\\sim Poisson\\left(  \\exp\\left(  \\sum_{k=1}^{K}\\beta_{k}\\phi\n",
    "_{xy}^{k}-u_{x}-v_{y}\\right)  \\right)  .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson regression with fixed effects\n",
    "\n",
    "Let $\\theta=\\left(  \\beta,u,v\\right)  $ and $Z=\\left(  \\phi,D^{x},D^{y}\\right)  $ where $D_{x^{\\prime}y^{\\prime}}^{x}=1\\left\\{  x=x^{\\prime}\\right\\}  $ and $D_{x^{\\prime}y^{\\prime}}^{y}=1\\left\\{  y=y^{\\prime}\\right\\}$ are $x$-and $y$-dummies. Let $m_{xy}\\left(  Z;\\theta\\right)  =\\exp\\left(\\theta^{\\intercal}Z_{xy}\\right)  $ be the parameter of the Poisson distribution.\n",
    "\n",
    "The conditional likelihood of $\\hat{\\pi}_{xy}$ given $Z_{xy}$ is\n",
    "\n",
    "\\begin{align*}\n",
    "l_{xy}\\left(  \\hat{\\pi}_{xy};\\theta\\right)   &  =\\hat{\\pi}_{xy}\\log m_{xy}\\left(  Z;\\theta\\right)  -m_{xy}\\left(  Z;\\theta\\right) \\\\\n",
    "&  =\\hat{\\pi}_{xy}\\left(  \\theta^{\\intercal}Z_{xy}\\right)  -\\exp\\left(\\theta^{\\intercal}Z_{xy}\\right) \\\\\n",
    "&  =\\hat{\\pi}_{xy}\\left(  \\sum_{k=1}^{K}\\beta_{k}\\phi_{xy}^{k}-u_{x}-v_{y}\\right)  -\\exp\\left(  \\sum_{k=1}^{K}\\beta_{k}\\phi_{xy}^{k}-u_{x}-v_{y}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Summing over $x$ and $y$, the sample log-likelihood is\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{xy}\\hat{\\pi}_{xy}\\sum_{k=1}^{K}\\beta_{k}\\phi_{xy}^{k}-\\sum_{x}p_{x}u_{x}-\\sum_{y}q_{y}v_{y}-\\sum_{xy}\\exp\\left(  \\sum_{k=1}^{K}\\beta_{k}\\phi_{xy}^{k}-u_{x}-v_{y}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "hence we recover the [objective function](#objFun)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Poisson to pseudo-Poisson\n",
    "\n",
    "If $\\pi_{xy}|xy$ is Poisson, then $\\mathbb{E}\\left[\\pi_{xy}\\right]=m_{xy}\\left(  Z_{xy};\\theta\\right)  =\\mathbb{V}ar\\left(  \\pi_{xy}\\right)  $. While it makes sense to assume the former equality, the latter is a rather strong assumption.\n",
    "\n",
    "For estimation purposes, $\\hat{\\theta}$ is obtained by\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\theta}\\sum_{xy}l\\left(  \\hat{\\pi}_{xy};\\theta\\right)  =\\sum_{xy}\\left(\\hat{\\pi}_{xy}\\left(  \\theta^{\\intercal}Z_{xy}\\right)  -\\exp\\left(\\theta^{\\intercal}Z_{xy}\\right)  \\right)\n",
    "\\end{align*}\n",
    "\n",
    "however, for inference purposes, one shall not assume the Poisson distribution. Instead\n",
    "\n",
    "\\begin{align*}\n",
    "\\sqrt{N}\\left(  \\hat{\\theta}-\\theta\\right)  \\Longrightarrow\\left(A_{0}\\right)  ^{-1}B_{0}\\left(  A_{0}\\right)  ^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "where $N=\\left\\vert \\mathcal{X}\\right\\vert \\times\\left\\vert \\mathcal{Y}\\right\\vert $ and $A_{0}$ and $B_{0}$ are estimated by\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{A}_{0}  &  =N^{-1}\\sum_{xy}D_{\\theta\\theta}^{2}l\\left(  \\hat{\\pi}_{xy};\\hat{\\theta}\\right)  =N^{-1}\\sum_{xy}\\exp\\left(  \\hat{\\theta}^{\\intercal}Z_{xy}\\right)  Z_{xy}Z_{xy}^{\\intercal}\\\\\n",
    "\\hat{B}_{0}  &  =N^{-1}\\sum_{xy}\\left(  \\hat{\\pi}_{xy}-\\exp\\left(  \\hat{\\theta}^{\\intercal}Z_{xy}\\right)  \\right)  ^{2}Z_{xy}Z_{xy}^{\\intercal}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gravity equation\n",
    "\n",
    "\"Structural gravity equation\" (Anderson and van Wincoop, 2003) as exposited in Head and Mayer (2014)\n",
    "handbook chapter:\n",
    "\n",
    "\\begin{align*}\n",
    "X_{ni}=\\underset{S_{i}}{\\underbrace{\\frac{Y_{i}}{\\Omega_{i}}}}\\underset{M_{n}}{\\underbrace{\\frac{X_{n}}{\\Psi_{n}}}}\\Phi_{ni}%\n",
    "\\end{align*}\n",
    "\n",
    "where $n$=importer, $i$=exporter, $X_{ni}$=trade flow from $i$ to $n$, $Y_{i}=\\sum_{n}X_{ni}$ is value of production, $X_{n}=\\sum_{i}X_{ni}$ is importers' expenditures, and $\\phi_{ni}$=bilateral accessibility of $n$ to $i$.\n",
    "\n",
    "$\\Omega_{i}$ and $\\Psi_{n}$ are \\textquotedblleft multilateral resistances\\textquotedblright, satisfying the set of implicit equations\n",
    "\n",
    "\\begin{align*}\n",
    "\\Psi_{n}=\\sum_{i}\\frac{\\Phi_{ni}Y_{i}}{\\Omega_{i}}\\text{ and }\\Omega_{i}%\n",
    "=\\sum_{n}\\frac{\\Phi_{ni}X_{n}}{\\Psi_{n}}%\n",
    "\\end{align*}\n",
    "\n",
    "These are exactly the same equations as those of the regularized OT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining trade\n",
    "\n",
    "Parameterize $\\Phi_{ni}=\\exp\\left(  \\sum_{k=1}^{K}\\beta_{k}D_{ni}^{k}\\right)  $, where the $D_{ni}^{k}$ are $K$ pairwise measures of distance between $n$ and $i$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "X_{ni}=\\exp\\left(  \\sum_{k=1}^{K}\\beta_{k}D_{ni}^{k}-s_{i}-m_{n}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where fixed effects $s_{i}=-\\ln S_{i}$ and $m_{n}=-\\ln M_{n}$ are adjusted by\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i}X_{ni}=Y_{i}\\text{ and }\\sum_{n}X_{ni}=X_{n}.\n",
    "\\end{align*}\n",
    "\n",
    "Standard choices of $D_{ni}^{k}$'s:\n",
    "\n",
    "* Logarithm of bilateral distance between $n$ and $i$\n",
    "\n",
    "* Indicator of contiguous borders; of common official language; of\n",
    "colonial ties\n",
    "\n",
    "* Trade policy variables: presence of a regional trade agreement; tariffs\n",
    "\n",
    "* Could include many other measures of proximity, e.g. measure of genetic/cultural distance, intensity of communications, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will solve this model by fixing a $\\beta$ and solving the matching problem using IPFP. Then in an outer loop we will solve for the $\\beta$ which minimizes the distance between model and empirical moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed =  12.56294560432434 s.\n"
     ]
    }
   ],
   "source": [
    "sigma = 1\n",
    "maxiterIpfp = 1000\n",
    "maxiter = 500\n",
    "tolIpfp = 1e-12\n",
    "tolDescent = 1e-12\n",
    "t_s = 0.03\n",
    "iterCount = 0\n",
    "contIter = True\n",
    "\n",
    "v_it = np.zeros((nbi, nbt))\n",
    "beta_k = np.repeat(0, nbk)\n",
    "\n",
    "thegrad = np.repeat(0, nbk)\n",
    "pi_nit = []\n",
    "\n",
    "theval_old = -math.inf\n",
    "\n",
    "ptm = time.time()\n",
    "while(contIter):\n",
    "    \n",
    "    #print(\"Iteration\", iterCount)\n",
    "    \n",
    "    for t in range(nbt):\n",
    "        \n",
    "        #print(\"Year\", t)\n",
    "\n",
    "        D_ij_k = Dnikt[t]\n",
    "\n",
    "        Phi = D_ij_k.dot(beta_k.reshape(nbk,1)).reshape(nbi,nbi)\n",
    "\n",
    "        contIpfp = True\n",
    "        iterIpfp = 0\n",
    "\n",
    "        v = v_it[:, t].reshape(1,nbi)\n",
    "        f = f_nit[t]\n",
    "        g = g_nit[t]\n",
    "\n",
    "        K = np.exp(Phi/sigma)\n",
    "        np.fill_diagonal(K,0)\n",
    "\n",
    "        fK = np.multiply(f,K)\n",
    "        gK = np.multiply(g,K)\n",
    "\n",
    "        while(contIpfp):\n",
    "\n",
    "            iterIpfp = iterIpfp + 1\n",
    "\n",
    "            u = sigma * np.log(np.sum(np.multiply(gK,np.exp((-IX.dot(v))/sigma)), axis = 1)).flatten()\n",
    "            vnext = sigma * np.log(np.sum(np.multiply(fK,np.exp((-u.T.dot(tIY))/sigma)), axis = 0))\n",
    "            error = np.max(np.abs(np.sum(np.multiply(gK,np.exp((-IX.dot(vnext) - u.T.dot(tIY))/sigma)), axis = 1) - 1))\n",
    "\n",
    "            if (error < tolIpfp or iterIpfp >= maxiterIpfp):\n",
    "                contIpfp = False\n",
    "            v = vnext\n",
    "\n",
    "        v_it[:,t] = np.asarray(v)[0]\n",
    "\n",
    "        fgK = np.multiply(f,gK)\n",
    "        pi_nit.append(np.multiply(fgK,np.exp((-IX.dot(v) - u.T.dot(tIY))/sigma)))\n",
    "\n",
    "        thegrad = thegrad + (pi_nit[t]-pihat_nit[t]).flatten(order = 'F').dot(D_ij_k)\n",
    "\n",
    "    beta_k = beta_k - t_s * thegrad\n",
    "\n",
    "    nonzero_pi_nit = np.concatenate(pi_nit).ravel()[np.where(np.concatenate(pi_nit).ravel()>0, True, False)]\n",
    "    theval = float(np.sum(np.multiply(thegrad,beta_k), axis = 1)) - sigma * float(np.sum(np.multiply(nonzero_pi_nit, np.log(nonzero_pi_nit)),axis=(0,1)))\n",
    "\n",
    "    iterCount = iterCount + 1\n",
    "\n",
    "    if (iterCount > maxiter or np.abs(theval - theval_old) < tolDescent):\n",
    "        contIter = False\n",
    "\n",
    "    theval_old = theval\n",
    "    thegrad = np.repeat(0, nbk)\n",
    "    pi_nit = []\n",
    "    \n",
    "diff = time.time() - ptm\n",
    "print('Time elapsed = ', diff, 's.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84092368  0.43744866  0.2474767  -0.22249036]]\n"
     ]
    }
   ],
   "source": [
    "beta_k = beta_k/sdD_k\n",
    "print(beta_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recover the PPML estimates on Table 1 p. 42 of [Yotov et al.'s book](https://www.wto.org/english/res_e/booksp_e/advancedwtounctad2016_e.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
